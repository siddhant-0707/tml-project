\documentclass[10pt,letterpaper]{article}

\usepackage[letterpaper,margin=0.75in,nohead]{geometry}

\usepackage[colorlinks]{hyperref}
\usepackage{url}
\usepackage{breakurl}
\usepackage{booktabs}  % For better table formatting
\usepackage{amssymb}  % For checkmark symbol
\usepackage{graphicx}  % For including images
\usepackage{float}  % For H placement option

\hypersetup{
    colorlinks,
    linkcolor={red},
    citecolor={red},
    urlcolor={blue}
}

% add packages as needed


\title{CIS 6261: Trustworthy Machine Learning\\
	\Large Mid-Semester Project Report \\ \textcolor{magenta}{[Option 1]: A Unified Inference-Time Defense for Adversarial Robustness and Privacy Protection}}

%% TODO: your name and email go here (all members of the group --- add/remove names based on the size of the group)
%% Comment out as needed and designate a point of contact
\author{
        Siddhant Chauhan {\em (Point of Contact)}\\
        \texttt{siddhant.chauhan@ufl.edu}\\
        \and
        Samarth Vinayaka\\
        \texttt{samarthvinayaka@ufl.edu}\\
        \and
        Pulkit Garg\\
        \texttt{pulkit.garg@ufl.edu}\\
        \and
        Shreyansh Nayak\\
        \texttt{nayak.sh@ufl.edu}\\
}

% set the date to today
\date{\today}


\begin{document} % start document tag

\maketitle


%%% Remember: writing counts! (try to be clear and concise.)
%%% Make sure to cite your sources and references (use refs.bib and \cite{} or \footnote{} for URLs).
%%%
%%% The mid-semester project report should be 2+ page (in 10pt font)

%% TODO: write a few paragraphs explaining what your project is about.
%%% 
%% -- For option 1 (recommended) you can just briefly state the goals and talk about your proposed approach.
%%%
%% -- For option 2 (self-directed) you should explain what the problem is and what your proposed solution is.
%%%
\section{Introduction}

This project focuses on developing defense techniques to protect machine learning models against two critical security threats: adversarial examples and membership inference attacks (MIAs). Our goal is to enhance the robustness and privacy of a pre-trained ResNet-18 model on CIFAR-10 while maintaining high prediction accuracy.

\textbf{Project Goals:}
\begin{itemize}
    \item \textbf{Adversarial Robustness:} Protect the model against adversarial examples that can cause misclassification with imperceptible perturbations. The baseline model shows a dramatic drop from 91\% benign accuracy to only 6\% adversarial accuracy, indicating severe vulnerability.
    \item \textbf{Privacy Protection:} Defend against membership inference attacks that attempt to determine whether a specific data sample was used during training. The baseline model exhibits an MIA advantage of approximately 0.31, indicating significant privacy leakage.
    \item \textbf{Maintain Accuracy:} Achieve these defenses without substantially degrading the model's performance on legitimate test data.
\end{itemize}

\textbf{Approach:} We implement an inference-time defense that combines multiple techniques:
\begin{enumerate}
    \item \textbf{Temperature Scaling:} Reduces the confidence gap between member and non-member predictions, making it harder for attackers to distinguish training data~\cite{guo2017calibration}. This technique smooths the output distribution and has been shown to help with calibration and privacy.
    \item \textbf{Test-Time Augmentation (TTA):} Applies input randomization, random crops, and flips to create an ensemble of predictions, improving robustness against adversarial perturbations~\cite{xie2018adversarial}. This approach leverages the observation that adversarial examples are often sensitive to input transformations.
    \item \textbf{Ensemble Prediction:} Averages predictions across multiple augmented views to stabilize outputs and reduce sensitivity to adversarial noise.
\end{enumerate}

This approach is particularly suitable for Part 1 of the project since we cannot retrain the model but can modify the prediction function at inference time. Membership inference attacks exploit the fact that models typically have higher confidence on training data than test data~\cite{shokri2017membership}, and our defense addresses this vulnerability directly.

\section{Literature Search and Approach Design}

\subsection{Literature Review}

To design an effective defense, we conducted a systematic literature review focusing on inference-time defense techniques that do not require model retraining. Our review was informed by papers discussed in course lectures, particularly those on membership inference attacks and adversarial examples, as well as additional relevant research in the field.

\subsubsection{Threat Landscape Analysis}

\textbf{Membership Inference Attacks:} The foundational work by Shokri et al.~\cite{shokri2017membership} established that models leak membership information through prediction confidence, with training examples typically exhibiting higher confidence than test examples. Recent research has shown that even label-only attacks can succeed~\cite{choquette2021label}, and enhanced attacks exploit multiple signals beyond simple confidence thresholds~\cite{ye2022enhanced}. These findings indicate that effective defenses must address multiple attack vectors simultaneously.

\textbf{Adversarial Examples:} Adversarial examples exploit the model's sensitivity to small input perturbations, with attacks ranging from single-step methods (FGSM~\cite{goodfellow2014explaining}) to sophisticated multi-step attacks (PGD~\cite{madry2017towards}). Critically, Athalye et al.~\cite{athalye2018obfuscated} demonstrated that many gradient-obfuscating defenses can give a false sense of security, as adaptive attackers can circumvent them. This emphasizes the importance of evaluating defenses against multiple attack methods.

\subsubsection{Defense Strategy Evaluation}

Given the constraint that we cannot retrain the model (Part 1 requirement), we systematically evaluated inference-time defense strategies. Table~\ref{tab:defense-strategies} summarizes our analysis of defense approaches for each threat.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Defense Strategy} & \textbf{MIA Defense} & \textbf{Adversarial Defense} \\
\midrule
Temperature Scaling & $\checkmark$ & $\times$ \\
Test-Time Augmentation & $\times$ & $\checkmark$ \\
Differential Privacy & $\checkmark$ & $\times$ \\
Adversarial Training & $\times$ & $\checkmark$ \\
Combined Approach & $\checkmark$ & $\checkmark$ \\
\bottomrule
\end{tabular}
\caption{Defense Strategy Coverage Analysis}
\label{tab:defense-strategies}
\end{table}

\textbf{Membership Inference Defenses:} We identified three primary categories: (1) confidence masking through output smoothing, (2) differential privacy with training-time noise injection~\cite{abadi2016deep}, and (3) output perturbation at inference time. Among these, confidence masking via temperature scaling~\cite{guo2017calibration} emerged as particularly promising because it preserves prediction order, is well-studied for model calibration, and reduces the confidence gap between members and non-members.

\textbf{Adversarial Defenses:} The landscape includes: (1) adversarial training~\cite{goodfellow2014explaining, madry2017towards}, which is the gold standard but requires training modifications, (2) input transformation techniques that preprocess inputs with random transformations, and (3) ensemble methods that average predictions across multiple views. Test-time augmentation with random transformations~\cite{xie2018adversarial} effectively breaks adversarial perturbations by making the exact input seen by the model unpredictable.

\subsubsection{Key Insight: Synergistic Combination}

Our literature review revealed that while many defenses address either privacy or adversarial robustness, few provide unified protection. Critically, we recognized that temperature scaling and test-time augmentation could be combined synergistically:

\begin{itemize}
    \item Temperature scaling not only protects privacy but can also help with robustness by smoothing predictions
    \item Input augmentation not only protects against adversarial examples but also adds randomness that can help with privacy
    \item Ensemble prediction provides stability and reduces variance, benefiting both accuracy and robustness
\end{itemize}

This insight formed the foundation of our combined defense approach.

\subsection{Brainstorming and Approach Selection}

Our brainstorming process systematically evaluated defense options that could address both threats simultaneously while working within Part 1 constraints (no model retraining). We enumerated all feasible inference-time defense strategies and evaluated each against our requirements.

\subsubsection{Systematic Evaluation of Defense Options}

We considered four primary defense strategies, evaluating each against three criteria: (1) effectiveness against MIA, (2) effectiveness against adversarial examples, and (3) impact on model accuracy.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Defense Strategy} & \textbf{MIA} & \textbf{Adversarial} & \textbf{Decision} \\
\midrule
Output Noise Addition & Partial & $\times$ & Rejected \\
Input Preprocessing Only & $\times$ & $\checkmark$ & Rejected \\
Temperature Scaling Only & $\checkmark$ & $\times$ & Rejected \\
Combined Approach & $\checkmark$ & $\checkmark$ & \textbf{Selected} \\
\bottomrule
\end{tabular}
\caption{Defense Strategy Evaluation Matrix}
\label{tab:defense-evaluation}
\end{table}

\textbf{Strategy 1: Output Noise Addition.} This approach adds random noise to predictions to obscure membership signals. \textit{Evaluation:} While potentially reducing membership leakage, preliminary analysis suggested significant accuracy degradation and limited adversarial protection. \textit{Decision:} Rejected.

\textbf{Strategy 2: Input Preprocessing Only.} This approach applies transformations (random crops, flips, noise) solely for adversarial defense. \textit{Evaluation:} Strong empirical support for adversarial robustness~\cite{xie2018adversarial}, but does not address membership inference attacks that exploit output confidence. \textit{Decision:} Rejected.

\textbf{Strategy 3: Temperature Scaling Only.} This approach applies temperature scaling solely for privacy protection. \textit{Evaluation:} Effectively reduces confidence gap between members and non-members~\cite{guo2017calibration}, but does not protect against adversarial examples. \textit{Decision:} Rejected.

\textbf{Strategy 4: Combined Approach.} This approach integrates temperature scaling with test-time augmentation and ensemble prediction. \textit{Evaluation:} Addresses both threats through complementary mechanisms. \textit{Decision:} Selected.

\subsubsection{Selection Rationale}

We selected the combined approach based on the following evaluation criteria:

\begin{enumerate}
    \item \textbf{Dual Protection:} Provides defense against both membership inference (temperature scaling) and adversarial examples (input transformation + ensemble)
    
    \item \textbf{Synergistic Effects:} Temperature scaling helps both privacy and robustness; input augmentation helps both robustness and privacy
    
    \item \textbf{Inference-Time Only:} Works entirely at inference time, perfect for Part 1 constraints
    
    \item \textbf{Literature Support:} Both techniques have strong empirical support~\cite{guo2017calibration, xie2018adversarial}
    
    \item \textbf{Adaptive Attack Resilience:} Addresses concerns raised by Athalye et al.~\cite{athalye2018obfuscated} by using input-level transformations rather than gradient masking
\end{enumerate}

\subsubsection{Design Decisions}

\textbf{Temperature Scaling:} Selected over other output perturbation methods because it:
\begin{itemize}
    \item Preserves prediction order (most likely class remains the same)
    \item Is well-studied for model calibration~\cite{guo2017calibration}
    \item Provides tunable control via temperature parameter
\end{itemize}

\textbf{Test-Time Augmentation:} Selected transformations (Gaussian noise, horizontal flips, random crops) because they:
\begin{itemize}
    \item Preserve semantic content (standard in image classification)
    \item Effectively break adversarial perturbations~\cite{xie2018adversarial}
    \item Add minimal computational overhead
\end{itemize}

\textbf{Ensemble Prediction:} Average logits (before softmax) rather than probabilities to:
\begin{itemize}
    \item Preserve temperature scaling effects
    \item Maintain numerical stability
    \item Allow temperature scaling after aggregation (more effective)
\end{itemize}

\textbf{Parameterization:} Designed with three tunable parameters (temperature, num\_samples, noise\_scale) to enable systematic optimization of the privacy-robustness-accuracy trade-off.

\subsubsection{Defense Mechanisms}

The defense operates through three complementary mechanisms:

\begin{enumerate}
    \item \textbf{Privacy Protection:} Temperature scaling reduces the confidence gap between members and non-members by smoothing the output distribution, making membership inference based on prediction confidence significantly more difficult.
    
    \item \textbf{Adversarial Robustness:} Input transformations break adversarial perturbations because adversarial examples are crafted for specific clean inputs; random transformations make the adversarial noise less effective or counterproductive.
    
    \item \textbf{Accuracy Preservation:} Ensemble prediction stabilizes outputs by averaging across multiple views, reducing variance and potentially improving accuracy on clean data while reducing the impact of adversarial noise.
\end{enumerate}

This literature-informed approach provides a principled, well-justified defense that addresses both threats while working within the constraints of Part 1. The subsequent parameter optimization (discussed in Section~\ref{sec:param-opt}) further refines the defense to achieve optimal performance.


%% TODO: write a few paragraphs detailing your progress so far.
%% Make sure to address the following:
%% - What have you done so far?
%% - Do you have any preliminary results, if so what are they? If possible include figures or tables.
%%
\section{Progress \& Preliminary Results}

\subsection{Implementation}

We have successfully implemented a defended prediction function in \texttt{part1.py} that wraps the original model's forward pass. The defense function applies the following transformations:

\begin{itemize}
    \item \textbf{Input Randomization:} Adds small Gaussian noise ($\sigma = 0.02$) to inputs and applies random horizontal flips and small random crops (2-pixel padding) to create diversity in the input space.
    \item \textbf{Ensemble Aggregation:} Generates 4 augmented versions of each input and averages the logits across all augmentations.
    \item \textbf{Temperature Scaling:} Divides the ensemble logits by a temperature parameter ($T = 2.0$) to smooth the output distribution and reduce confidence-based privacy leakage.
\end{itemize}

The defense is activated by setting \texttt{defense\_enabled = True} in the main evaluation code, allowing easy comparison between defended and undefended models.

\subsection{Baseline Results (Undefended Model)}

Before implementing the defense, we evaluated the baseline model performance:

\begin{itemize}
    \item \textbf{Accuracy:} Train: 93.43\%, Validation: 85.74\%
    \item \textbf{Membership Inference:} Simple confidence threshold MIA achieved 65.23\% attack accuracy with 0.305 advantage; logits threshold MIA achieved 65.43\% attack accuracy with 0.309 advantage.
    \item \textbf{Adversarial Robustness:} Benign accuracy: 91.00\%, Adversarial accuracy: 6.00\% (against Attack0)
\end{itemize}

These results confirm significant vulnerabilities: the model is highly susceptible to adversarial examples and leaks substantial information about training data membership.

\subsection{Defended Model Results}

After implementing our defense, we observed the following improvements:

\begin{itemize}
    \item \textbf{Accuracy:} Train: 94.07\% (+0.64\%), Validation: 86.46\% (+0.72\%)
    \item \textbf{Membership Inference:} Multiple MIA attacks evaluated with varying effectiveness (see detailed results below)
    \item \textbf{Adversarial Robustness:} Significant improvements across all attack methods (see detailed results below)
\end{itemize}

\subsection{Detailed MIA Attack Results}

We evaluated our defense against 7 different membership inference attacks:

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Attack Method} & \textbf{Attack Acc.} & \textbf{Advantage} & \textbf{F1 Score} \\
\midrule
Simple Conf threshold MIA & 50.00\% & 0.000 & 0.000 \\
Simple Logits threshold MIA & 50.00\% & 0.000 & 0.000 \\
Entropy-based MIA & 58.98\% & 0.180 & 0.590 \\
Adaptive Conf threshold MIA & 54.10\% & 0.082 & 0.388 \\
Likelihood ratio MIA & 50.00\% & 0.000 & 0.667 \\
Loss-based MIA (cross\_entropy) & 41.89\% & -0.162 & 0.418 \\
Loss-based MIA (entropy) & 58.30\% & 0.166 & 0.583 \\
\bottomrule
\end{tabular}
\caption{Membership Inference Attack Results on Defended Model}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Perfect Defense:} Three attacks (Simple Conf threshold, Simple Logits threshold, Likelihood ratio) achieve exactly 50\% accuracy with 0.000 advantage, indicating they perform no better than random guessing.
    \item \textbf{Strong Defense:} Adaptive Conf threshold MIA shows minimal leakage (0.082 advantage), representing an 74\% reduction from the baseline (0.31 advantage).
    \item \textbf{Partial Defense:} Entropy-based and Loss-based (entropy) attacks show moderate leakage (0.180 and 0.166 advantage respectively), indicating these attacks are more sophisticated but still significantly reduced from baseline.
    \item \textbf{Over-Defense:} Loss-based MIA (cross\_entropy) achieves negative advantage (-0.162), meaning the defense actually reverses the attack signal, making non-members appear more like members than actual members.
\end{itemize}

\subsection{Detailed Adversarial Attack Results}

We evaluated our defense against 4 different adversarial attacks, including the provided Attack0 and three newly implemented attacks:

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Attack Method} & \textbf{Benign Acc.} & \textbf{Adversarial Acc.} & \textbf{Drop} \\
\midrule
Attack0 (provided) & 90.75\% & 71.25\% & -19.50\% \\
FGSM & 89.69\% & 50.16\% & -39.53\% \\
PGD & 89.38\% & 59.84\% & -29.54\% \\
BIM & 90.00\% & 52.34\% & -37.66\% \\
\bottomrule
\end{tabular}
\caption{Adversarial Attack Results on Defended Model}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Baseline Comparison:} The undefended model had 6\% adversarial accuracy against Attack0. Our defense improves this to 71.25\%, representing an 11.9x improvement.
    \item \textbf{Strongest Attack:} FGSM causes the largest accuracy drop (39.53\%), but still maintains 50.16\% accuracy, which is substantially better than the 6\% baseline.
    \item \textbf{Multi-step Attacks:} PGD and BIM (both iterative attacks) achieve 59.84\% and 52.34\% adversarial accuracy respectively, demonstrating that our defense provides meaningful protection against stronger iterative attacks.
    \item \textbf{Attack0 Performance:} The provided Attack0 is the weakest against our defense (71.25\% accuracy), suggesting it may be a simpler attack or one that our defense is particularly effective against.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth,keepaspectratio]{../examples/screenshots/part1-ss.png}
\caption{Complete evaluation output showing MIA attack results and adversarial attack results for the defended model.}
\label{fig:part1-results}
\end{figure}

\subsection{Analysis}

The defense demonstrates strong effectiveness across multiple attack vectors:

\begin{enumerate}
    \item \textbf{Privacy Protection:} With the optimized configuration (temperature=3.5), three out of seven MIA attacks are completely neutralized (0.000 advantage), and all attacks show significantly reduced effectiveness compared to the baseline (0.31 advantage). The average MIA advantage is 0.147 (52\% reduction from baseline), demonstrating strong privacy protection. The defense successfully mitigates confidence-based attacks through temperature scaling, though entropy-based attacks show some residual leakage that is still substantially reduced from baseline.
    
    \item \textbf{Adversarial Robustness:} With the optimized configuration, all adversarial attacks show substantial improvement from the 6\% baseline. The defense achieves 51.88\% to 73.00\% adversarial accuracy depending on the attack (average: 59.78\%), representing a 10x improvement from the baseline. Test-time augmentation and ensemble methods effectively improve robustness by creating multiple views of each input, with the optimal configuration using 4 samples providing the best balance.
    
    \item \textbf{Performance Impact:} With the optimized configuration, the defense maintains high model accuracy on clean data (86.76\% validation accuracy vs 85.74\% baseline), representing a slight improvement likely due to the ensemble effect reducing prediction variance. Benign accuracy remains high (89-91\%) across all adversarial attack evaluations, demonstrating that the defense does not significantly degrade performance on legitimate inputs.
    
    \item \textbf{Computational Cost:} With the optimized configuration (temperature=3.5, num\_samples=4, noise\_scale=0.03), the defense increases inference time from approximately 9.5 seconds to 25.6 seconds (about 2.7x slower), which is well within the 20-minute constraint specified in the project requirements. The optimized configuration uses 4 samples instead of 6, providing a good balance between protection and computational efficiency.
    
    \item \textbf{Attack Diversity:} By implementing multiple attack methods (7 MIA attacks and 4 adversarial attacks), we demonstrate that our defense is robust across different attack strategies, not just the specific attacks provided in the baseline evaluation.
\end{enumerate}

\subsection{Implementation of Additional Attacks}

Beyond the baseline attacks, we implemented several new attack methods to comprehensively evaluate our defense:

\textbf{Membership Inference Attacks:}
\begin{itemize}
    \item \textbf{Entropy-based MIA:} Uses prediction entropy as a signal (members have lower entropy)
    \item \textbf{Adaptive Conf threshold MIA:} Uses percentile-based adaptive threshold instead of fixed value
    \item \textbf{Likelihood ratio MIA:} Compares prediction confidence to a baseline threshold
    \item \textbf{Loss-based MIA:} Uses cross-entropy loss or prediction entropy as loss signal
\end{itemize}

\textbf{Adversarial Attacks:}
\begin{itemize}
    \item \textbf{FGSM (Fast Gradient Sign Method):} Single-step gradient-based attack with $\epsilon=0.03$
    \item \textbf{PGD (Projected Gradient Descent):} Multi-step iterative attack (10 iterations) with random start
    \item \textbf{BIM (Basic Iterative Method):} Multi-step iterative attack (10 iterations) without random start
\end{itemize}

All adversarial attacks were generated against the undefended model using the validation set and saved in the same format as the provided Attack0 (NCHW format, normalized values). The attacks are automatically evaluated if the corresponding files exist.

These comprehensive results suggest that our combined approach of temperature scaling and test-time augmentation is effective for both privacy protection and adversarial robustness across diverse attack methods, making it a promising and well-evaluated defense for the remainder of the project.

\subsection{Parameter Optimization}
\label{sec:param-opt}

After initial implementation, we conducted systematic parameter tuning to optimize defense performance. We developed two evaluation scripts: (1) a parameter impact analysis script that tests individual parameters while keeping others fixed, and (2) a comprehensive grid search script that evaluates all parameter combinations.

\textbf{Parameter Ranges Tested:}
\begin{itemize}
    \item \textbf{Temperature:} 1.0 to 4.0 (key values: 1.5, 2.0, 2.5, 3.0, 3.5, 4.0)
    \item \textbf{Num\_samples:} 1 to 10 (key values: 2, 4, 6, 8, 10)
    \item \textbf{Noise\_scale:} 0.0 to 0.05 (key values: 0.01, 0.02, 0.03, 0.04, 0.05)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth,keepaspectratio]{../examples/screenshots/tuning-ss.png}
\caption{Parameter impact analysis showing the effect of temperature, num\_samples, and noise\_scale on validation accuracy, MIA advantage, and adversarial accuracy. This systematic analysis informed our parameter optimization process and final defense configuration selection.}
\label{fig:tuning-results}
\end{figure}

\textbf{Key Findings from Parameter Analysis:}

\begin{enumerate}
    \item \textbf{Temperature Impact:} Higher temperatures (3.5-4.0) significantly improve privacy protection, reducing MIA advantage from ~0.18 to ~0.12-0.15. However, adversarial robustness shows more variability with temperature. Temperature 3.5 provides the best balance, achieving 69.75\% adversarial accuracy while maintaining good privacy protection (MIA advantage: 0.147).
    
    \item \textbf{Ensemble Size Impact:} Increasing num\_samples from 1 to 4 dramatically improves adversarial robustness (from 29.5\% to 71.25\%). Beyond 4 samples, improvements are marginal with diminishing returns. Num\_samples=4 represents the optimal balance between robustness and computational cost.
    
    \item \textbf{Noise Scale Impact:} Adding input noise (0.03-0.05) significantly improves adversarial robustness, with noise\_scale=0.05 achieving 73\% adversarial accuracy. However, noise\_scale=0.03 provides better balance, maintaining high clean accuracy while achieving good robustness (65.75\% adversarial accuracy).
\end{enumerate}

\textbf{Configuration Comparison:}

We evaluated two primary configurations:

\begin{table}[ht]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Privacy} & \textbf{Adversarial} & \textbf{Accuracy} \\
 & \textbf{(MIA Adv)} & \textbf{(Avg Acc)} & \textbf{(Val Acc)} \\
\midrule
Grid Search Balanced & 0.084 & 51.57\% & 0.8696 \\
(temp=1.5, num\_samples=6) & & & \\
\midrule
Optimized Balanced & 0.147 & 59.78\% & 0.8676 \\
(temp=3.5, num\_samples=4) & & & \\
\bottomrule
\end{tabular}
\caption{Configuration Comparison}
\end{table}

\textbf{Final Configuration Selection:}

After comprehensive evaluation, we selected the optimized balanced configuration (temperature=3.5, num\_samples=4, noise\_scale=0.03) for the following reasons:

\begin{itemize}
    \item \textbf{Superior Adversarial Robustness:} Achieves 59.78\% average adversarial accuracy (vs 51.57\% for grid search config), representing a 10x improvement from the 6\% baseline. This is critical for the project's adversarial defense goals.
    
    \item \textbf{Good Privacy Protection:} While MIA advantage is higher (0.147 vs 0.084), the defense still maintains perfect protection (0.000 advantage) on 3 out of 7 MIA attacks and reduces all attacks significantly from the baseline (0.31 advantage).
    
    \item \textbf{Computational Efficiency:} Uses 4 samples instead of 6, resulting in faster inference (25.6s vs 35.2s evaluation time) while maintaining strong protection.
    
    \item \textbf{Better Overall Balance:} Provides stronger adversarial robustness with acceptable privacy protection, better aligning with the project's dual goals of protecting against both threats.
\end{itemize}

\textbf{Optimized Performance Metrics:}

With the final configuration (temperature=3.5, num\_samples=4, noise\_scale=0.03):

\begin{itemize}
    \item \textbf{Privacy Protection:} Average MIA advantage of 0.147 (52\% reduction from 0.31 baseline), with 3 attacks completely neutralized (0.000 advantage).
    
    \item \textbf{Adversarial Robustness:} Average adversarial accuracy of 59.78\% across all attacks (Attack0: 73\%, FGSM: 56.56\%, PGD: 57.66\%, BIM: 51.88\%), representing a 10x improvement from the 6\% baseline.
    
    \item \textbf{Model Accuracy:} Validation accuracy of 86.76\%, maintaining high performance on clean data while providing strong defenses.
    
    \item \textbf{Computational Cost:} Evaluation time of 25.6 seconds, well within the 20-minute constraint and faster than alternative configurations.
\end{itemize}


%% TODO: write a few paragraphs about what you hope to accomplish by the end of the semester (reminder: final report is due the last week of classes)
%% Make sure to address the following:
%% - Are you on schedule? Do you need to reduce the scope?
%% - What do you expect you will have by the end of the semester?
%% 
\section{Next Steps}

\subsection{Additional Attack Evaluation}

We have successfully implemented and evaluated multiple additional attacks:

\textbf{Completed Attack Implementations:}
\begin{itemize}
    \item \textbf{Advanced MIA Attacks:} Implemented 5 additional MIA attacks (entropy-based, adaptive confidence threshold, likelihood ratio, and two loss-based attacks), bringing the total to 7 MIA attacks evaluated.
    \item \textbf{Stronger Adversarial Attacks:} Implemented and evaluated 3 additional adversarial attack methods (FGSM, PGD, BIM) beyond the provided Attack0, comprehensively evaluating robustness against diverse attack strategies.
    \item \textbf{Attack Generation:} All adversarial attacks were generated against the undefended model and saved for reproducible evaluation.
\end{itemize}

\textbf{Future Work:}
\begin{itemize}
    \item \textbf{Adaptive Attacks:} Test against adversaries who know about our defense and can adapt their attacks accordingly, which is crucial for realistic security evaluation. We are aware that gradient-based defenses can sometimes provide a false sense of security~\cite{athalye2018obfuscated}, so adaptive attack testing is an important next step.
    \item \textbf{Additional Attack Methods:} Consider implementing C\&W attack and AutoAttack for even more comprehensive evaluation.
    \item \textbf{Shadow Model Attacks:} Implement shadow model-based MIA attacks for more sophisticated privacy evaluation.
\end{itemize}

\subsection{Part 2: Dataset and Model Selection}

For Part 2, we conducted a systematic evaluation of datasets and model architectures to select an appropriate combination that meets the project requirements while allowing us to leverage our Part 1 work.

\subsubsection{Dataset Selection Process}

We evaluated several datasets against the project requirements:
\begin{itemize}
    \item \textbf{Complexity requirement:} Dataset must be at least as complex as CIFAR-10 (MNIST is too simple)
    \item \textbf{Similarity to CIFAR-10:} Prefer image datasets not too different from CIFAR-10 to leverage Part 1 work
    \item \textbf{Computational feasibility:} Must be trainable within available resources
\end{itemize}

\textbf{Datasets Considered:}

\begin{table}[ht]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Complexity} & \textbf{Code Reuse} & \textbf{Training Time} \\
\midrule
CIFAR-100 & \checkmark & \checkmark\checkmark\checkmark & Fast \\
ImageNette & \checkmark & \checkmark\checkmark & Medium \\
EuroSAT & \checkmark & \checkmark\checkmark & Fast \\
STL-10 & \checkmark & \checkmark\checkmark & Medium \\
\bottomrule
\end{tabular}
\caption{Dataset Comparison for Part 2}
\label{tab:dataset-comparison}
\end{table}

\textbf{Selected Dataset: ImageNette}

We selected \textbf{ImageNette} (a 10-class subset of ImageNet) for the following reasons:

\begin{enumerate}
    \item \textbf{Appropriate Complexity:} ImageNette contains 13,394 images across 10 classes with higher resolution (320×320) than CIFAR-10, providing a more challenging classification task while remaining computationally manageable.
    
    \item \textbf{Real-World Relevance:} As a subset of ImageNet, ImageNette represents real-world natural images, making it more representative of practical applications than synthetic datasets.
    
    \item \textbf{Training Efficiency:} The dataset size (9,469 training samples) allows for reasonable training times (~3-5 hours for 100 epochs on GPU) while still providing sufficient data for robust model training.
    
    \item \textbf{Defense Evaluation:} Higher resolution images (224×224 after preprocessing) provide a better testbed for evaluating defenses on more realistic inputs, while still allowing us to adapt our Part 1 defense techniques.
    
    \item \textbf{Well-Established Benchmark:} ImageNette is a standard benchmark in the fastai community, providing known baseline accuracies for comparison (typically 85-90\% for ResNet-18).
\end{enumerate}

\subsubsection{Model Architecture Selection}

We evaluated several model architectures for ImageNette:

\begin{table}[ht]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Architecture} & \textbf{Parameters} & \textbf{Expected Acc.} & \textbf{Training Time} \\
\midrule
ResNet-18 & 11.2M & 85-90\% & 3-5 hours \\
ResNet-34 & 21.3M & 88-92\% & 6-10 hours \\
EfficientNet-B0 & 5.3M & 85-88\% & 4-6 hours \\
MobileNet-V2 & 3.4M & 80-85\% & 2-4 hours \\
\bottomrule
\end{tabular}
\caption{Model Architecture Comparison}
\label{tab:model-comparison}
\end{table}

\textbf{Selected Architecture: ResNet-18}

We selected \textbf{ResNet-18} for the following reasons:

\begin{enumerate}
    \item \textbf{Proven Effectiveness:} ResNet-18 is a well-established architecture that achieves strong performance on ImageNette (typically 85-90\% accuracy), providing a solid baseline for defense evaluation.
    
    \item \textbf{Computational Efficiency:} With 11.2M parameters, ResNet-18 offers a good balance between model capacity and training time, allowing us to complete training and defense evaluation within the project timeline.
    
    \item \textbf{Standard Architecture:} ResNet-18 uses the standard ImageNet architecture (224×224 input), which is well-documented and allows for easy comparison with published results.
    
    \item \textbf{Defense Compatibility:} The architecture is compatible with our Part 1 defense techniques (temperature scaling, test-time augmentation, ensemble prediction) without requiring significant modifications.
    
    \item \textbf{Training-Time Defense Support:} ResNet-18 is well-suited for incorporating training-time defenses such as adversarial training and differential privacy, which we plan to explore in Part 2.
\end{enumerate}

\subsubsection{Initial Training Results}

We conducted an initial training run to verify the setup and baseline performance. Training for just 3 epochs with batch size 64 and learning rate 0.1, we achieved:

\begin{itemize}
    \item \textbf{Training Progress:} The model showed steady improvement, reaching 50.27\% validation accuracy after 3 epochs (from 24.69\% at epoch 1).
    \item \textbf{Training Time:} ~19 seconds per epoch on GPU, confirming computational feasibility.
    \item \textbf{Model Size:} 11,181,642 parameters, as expected for ResNet-18.
    \item \textbf{Dataset Statistics:} 9,469 training samples, 3,925 validation/test samples, 10 classes.
\end{itemize}

These preliminary results confirm that our selected dataset and architecture combination is appropriate for Part 2. With full training (100 epochs), we expect to achieve 85-90\% test accuracy, providing a strong baseline for evaluating our defense techniques.

\subsubsection{Next Steps for Part 2}

With the dataset and architecture selected, our next steps are:

\begin{enumerate}
    \item \textbf{Complete Model Training:} Train ResNet-18 on ImageNette for 100 epochs to achieve baseline performance (~85-90\% accuracy).
    
    \item \textbf{Apply Part 1 Defenses:} Adapt our inference-time defense techniques (temperature scaling, test-time augmentation, ensemble prediction) to work with the ImageNette model and evaluate their effectiveness.
    
    \item \textbf{Explore Training-Time Defenses:} Since we are training from scratch, we can incorporate training-time defenses such as adversarial training and differential privacy, which were not possible in Part 1.
    
    \item \textbf{Comprehensive Evaluation:} Evaluate the combined defense approach against multiple MIA and adversarial attacks, similar to our Part 1 evaluation.
\end{enumerate}


%%%%

\bibliography{refs}
\bibliographystyle{plain}


\end{document} % end tag of the document